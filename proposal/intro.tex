
The rise of large-scale software systems (e.g., Amazon.com and Google Gmail) has posed an impact on people's daily lives from mobile devices users to space station operators. The increasing importance and complexity of such systems make their quality a critical, yet extremely difficult issue to address. Failures in such systems are more often associated with performance issues, rather than with feature bugs~\cite{Weyuker:2000}. Therefore, performance assurance activities are an essential step in the release cycle of large software systems. 

Performance assurance activities aim to identify and eliminate performance regressions in each newly released version. Examples of performance regressions are response time degradation, higher than expected resource utilization and memory leaks. Such regressions may compromise the user experience, increase the operating cost of the system, and cause field failures. The slow response time of the United States' newly rolled-out healthcare.gov illustrates the importance of performance assurance activities before releasing a system. Failure in detecting such regressions would result in significant financial and reputational repercussions.

Prior software quality research typically focus on functional bugs rather performance issues. For example, post-release bugs are often used as code quality measurement and are modeled by statistical modeling techniques in order to understand the relationship between different software engineering activities and code quality~\cite{Hassan:2009:PFU}. In addition, bug prediction techniques are proposed to prioritize software quality assurance efforts~\cite{Zimmermann:2007:PDE,Nagappan:2005:URC,Nagappan:2006:MMP} and assess the risk of code changes~\cite{emadjit}. However, performance regressions are rarely targeted in spite of their importance.

On the other hand, prior study of JIT defect prediction focus more on the risk of commit based on bug rather than performance regressions. Predicting performance regressions remains a task that is conducted after the fact, i.e., after the system is built and deployed in the field or dedicated performance testing environments. However, large amounts of resources are required to detect, locate, understand and fix performance regressions at such a late stage in the development circle; while the amount of required resources would be significantly reduced if developers were notified whether a code change introduces performance regressions during development. 

In this research, we perform an empirical study on the JIT prediction of performance regression introducing code changes. 
By examining the identified performance regression introducing changes, we find that performance regression introducing changes are prevalent during software development. The identified performance regressions are often associated with complex syndrome, i.e., multiple performance metrics have performance regression. In order to build a change risk model to predict JIT performance regression, we combine the basic commit-level measures proposed by Mockus and Weiss \cite{mockus2000predicting}, which are based on the characteristics of code changes, such as the number of modified subsystems and the purpose of the code change with the performance related measures we added, such as changing conditions and passing expensive parameters.

%Interestingly, we find that performance regression introducing changes also improve performance at the same time. Such results show that developers may not be aware of the existence of performance regressions, even when they are trying to improve performance. By studying the context and root-causes of performance regression introducing changes, we find that performance regressions are mostly introduced while fixing other functional bugs. We identify six code level root-causes of performance regressions, where majority of the performance regressions are introduced due to adding expensive function calls.

%Our study results shed light on the characteristic of performance regression introducing changes and suggest the lack of awareness and systematic performance regression testing in practice. Based on our findings, performance-aware change impact analysis and designing inexpensive performance tests may help practitioners better mitigate the prevalent performance regression that are introduced during software development.

The rest of this proposal is organized as follows: Section~\ref{sec:related} presents the prior research that is related to this project. Section~\ref{sec:case} presents our subject systems and our approach to predicting performance regression introducing code changes. Section~\ref{sec:results} presents our research questions and result. Section~\ref{sec:threats} presents the threats to the validity of our study. Finally, Section~\ref{sec:conclusion} concludes this paper.

