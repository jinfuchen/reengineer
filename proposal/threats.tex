\subsection{External Validity}

\textbf{Generalizing our results. }In our case study, we only focus on fifteen releases from two open source systems, i.e., \emph{Hadoop} and \emph{RxJava}. Both of the subject systems are mainly written in \emph{Java} languages. Some of the findings might not be generalizable to other systems or other programming languages. Future studies may consider more releases from more systems and even different programming languages (such as C\#, C++). 

\textbf{The management structure of Hadoop.} Although \emph{Hadoop} and \emph{Rxjava} are both widely accepted open source projects, we find a gap between these two systems in their management structure. There exists a \emph{PMC} (Project Management Committee) level in \emph{Hadoop}, and this committee owns the authorization to select developers who have direct access to modify source code repository. Since \emph{Hadoop} is a widely adopted system all over the world, every change needs to be very cautious and under careful review, the overall source code quality is much higher than its peer open source systems. In that case, our model may produce slightly biased results when predicting some systems owned or developed by other smaller organizations. What's more, some contributors are not active anymore, that makes their information is not available, so we use global constants to fill the missing part. Although these missing values can affect our dataset slightly, developers' experience is proven as a minor factor and it will not cause influential alterations to our model.

\subsection{Internal Validity}

\textbf{Selection of performance metrics.} Our approach requires performance metrics to measuring performance. In particular, we pick one commonly used domain level and four commonly used physical level performance metrics based on the nature of the subject systems. There exist a large number of other performance metrics. However, practitioners may require system-specific expertise to select an appropriate set of performance metrics that are important to their specific software. Future work can include more performance metrics based on the characteristic of the subject systems. 

\textbf{Model building and validation.} 
Logistic regression model can give us the intuitive formula and interpret the prediction easily. However, most of time models based on other machine learning algorithms such as support vector machines (SVM) can achieve a higher and more stable precision. In our future work, we will employ more models to predict performance regression and compare them to the logistic regression model.

Simply using precision and recall as validation metrics may be misleading when evaluating the binary class. In our dataset, test case without regression accounts for high proportion so it makes class imbalance. The precision and recall will be not stable.  Further, we will utilize receiver operator characteristic (ROC) as a validation to eliminate this thread.
\subsection{Construct Validity}

\textbf{Monitoring performance of subject systems.} Our study is based on the ability to accurately monitor the performance of our subject systems. This is based on the assumption that the performance monitoring library, i.e. \emph{psutil} can successfully and accurately providing performance metrics. This tool monitoring library is widely used in performance engineering research~\cite{peterfse,tarekmsr16}. To further validate our findings, other performance monitoring platforms (such as PerfMon~\cite{perfmon}) can be used. 



