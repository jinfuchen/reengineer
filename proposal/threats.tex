\subsection{External Validity}

\textbf{Generalizing our results. }In our case study, we only focus on fifteen releases from two open source systems, i.e., \emph{Hadoop} and \emph{RxJava}. Both of the subject systems are mainly written in \emph{Java} languages. Some of the findings might not be generalizable to other systems or other programming languages. Future studies may consider more releases from more systems and even different programming languages (such as C\#, C++). 

\textbf{Management structure of Hadoop.} Althought \emph{Hadoop} and \emph{Rxjava} are both widely accepted open source projects, we find a gap between these two systems in their management strucutre. There exists a \emph{PMC} (Project Management Committe) level in \emph{Hadoop}, and this committe owns the authorization to select committers who have direct access to modify source code repository. Since \emph{Hadoop} is a widely adopted system all over the world, every change needs to be very cautious and under careful review, the overall source code quality is much higher than its peer open source systems. In that case, our model may produce slightly biased results when predicting some systems owned or developed by other smaller organizations. What's more, some contributors are not active anymore, that makes their information is not available, so we use global constants to fill the missing part. Although these missing values can affect our dataset slightly, developers' experience is proven as a minor factor and it will not cause influential alterations to our model.

\subsection{Internal Validity}

\textbf{Selection of performance metrics.} Our approach requires performance metrics to measuring performance. In particular, we pick one commonly used domain level and four commonly used physical level performance metrics based on the nature of the subject systems. There exist a large number of other performance metrics. However, practitioners may require system-specific expertise to select an appropriate set of performance metrics that are important to their specific software. Future work can include more performance metrics based on the characteristic of the subject systems. 

\subsection{Construct Validity}

\textbf{Monitoring performance of subject systems.} Our study is based on the ability to accurately monitor performance of our subject systems. This is based on the assumption that the performance monitoring library, i.e. \emph{psutil} can successfully and accurately providing performance metrics. This tool monitoring library is widely used in performance engineering research~\cite{peterfse,tarekmsr16}. To further validate our findings, other performance monitoring platforms (such as PerfMon~\cite{perfmon}) can be used. 

\textbf{Issue report types. }We depend on the types of issues that are associated with each performance regression introducing commit. The issue report type may not be entirely accurate. For example, developers include extra code changes in issue reports with type \emph{documentation}. Firehouse-style user studies~\cite{thomasfritzfirehouse} can be adopted to better understand the context of performance regression introducing changes. 



