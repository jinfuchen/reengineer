\subsection{External Validity}

\textbf{Generalizing our results. }In our case study, we only focus on fifteen releases from two open source systems, i.e., \emph{Hadoop} and \emph{RxJava}. Both of the subject systems are mainly written in \emph{Java} languages. Some of the findings might not be generalizable to other systems or other programming languages. Future studies may consider more releases from more systems and even different programming languages (such as C\#, C++). 

\subsection{Internal Validity}

\textbf{Selection of performance metrics.} Our approach requires performance metrics to measuring performance. In particular, we pick one commonly used domain level and four commonly used physical level performance metrics based on the nature of the subject systems. There exist a large number of other performance metrics. However, practitioners may require system-specific expertise to select an appropriate set of performance metrics that are important to their specific software. Future work can include more performance metrics based on the characteristic of the subject systems. 

\textbf{Model validation.}

\subsection{Construct Validity}

\textbf{Monitoring performance of subject systems.} Our study is based on the ability to accurately monitor performance of our subject systems. This is based on the assumption that the performance monitoring library, i.e. \emph{psutil} can successfully and accurately providing performance metrics. This tool monitoring library is widely used in performance engineering research~\cite{peterfse,tarekmsr16}. To further validate our findings, other performance monitoring platforms (such as PerfMon~\cite{perfmon}) can be used. 



