\subsection{External Validity}

\textbf{Generalizing our results. }In our case study, we only focus on fifteen releases from two open source systems, i.e., \emph{Hadoop} and \emph{RxJava}. Both of the subject systems are mainly written in \emph{Java} languages. Some of the findings might not be generalizable to other systems or other programming languages. Future studies may consider more releases from more systems and even different programming languages (such as C\#, C++). 

\subsection{Internal Validity}

\textbf{Selection of performance metrics.} Our approach requires performance metrics to measuring performance. In particular, we pick one commonly used domain level and four commonly used physical level performance metrics based on the nature of the subject systems. There exist a large number of other performance metrics. However, practitioners may require system-specific expertise to select an appropriate set of performance metrics that are important to their specific software. Future work can include more performance metrics based on the characteristic of the subject systems. 

\textbf{Model building and validation.} 
Logistic regerssion model can give us intuitive formula and  interpret the predition easily. However, most of time models based on other machine learning algorithms such as support vector machines (SVM) can achieve a higner and more stable precision. In our future work, we will employ more models to predict performance regression and compare them to logistic regression model.

Simply using precision and recall as validation metrics may be misleading when evaluating binary class. In our dataset, test case without regression accounts for high propotion so it makes class imbalance. The precision and recall will be not stable.  Further, we will utilize receiver operator characteristic (ROC) as a validation to eliminate this thread.
\subsection{Construct Validity}

\textbf{Monitoring performance of subject systems.} Our study is based on the ability to accurately monitor performance of our subject systems. This is based on the assumption that the performance monitoring library, i.e. \emph{psutil} can successfully and accurately providing performance metrics. This tool monitoring library is widely used in performance engineering research~\cite{peterfse,tarekmsr16}. To further validate our findings, other performance monitoring platforms (such as PerfMon~\cite{perfmon}) can be used. 



