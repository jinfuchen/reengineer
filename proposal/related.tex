In this section, we present the related prior research to this paper in three aspects: 1) current state-of-the-art performance regression detection, 2) Prediction of JIT software defect and 3) empirical study on performance.

%\subsection{Motivation}
%As software evolves problematic code changes can significantly degrade software performance. Performance regression affects the quality and use of software so we have to find the root cause of problematic code change and fix the performance bugs. Performance regression testing can detect the bugs but the testing is resource consuming and high cost. The goal of our research is to detect the code changes which cause performance regression and classify what kind of performance bug the problematic codes cause. At the same time, we need improve performance regression testing efficiency through skip non-regressing commits and test case reduction.
\subsection{Performance regression detection}
A great amount of research has been proposed to detect performance regression.

Ad hoc analysis selects a limited number of target performance counters (e.g., CPU and memory) and performs simple analysis to compare the target counters. Heger et al.~\cite{Heger:2013:ARC} present an approach to support software engineers with root cause analysis of the problems. Their approach combines the concepts of regression testing, bisection and call tree analysis to detect performance regression root cause analysis as early as possible.

Pair-wise analysis compares and analyzes the performance metrics between two consecutive versions of a system to detect the problem. Nguyen et al.~\cite{Nguyen:2012:ADP,nguyen2011automated,Nguyen:2014:ICS} conduct a series of studies on performance regressions. Nguyen et al. propose an approach to detect performance regression by using a statistical process control technique called control charts. They construct the control chart and apply it to detect performance regressions and examine the violation ratio of the same performance counter. Malik et al.~\cite{Malik:2013:ADP} propose approaches that combine one supervised and three unsupervised algorithms to help performance regression detection. They employ feature selection methods named Principal Component Analysis (PCA) to reduce the dimensionality of the observed performance counter set and validate their approach through a large case study on a real-world industrial software system~\cite{malik2010automatic}.

Model-based analysis builds a limited number of detected models for a set of target performance counters (e.g., CPU and memory) and leverages the models to detect performance regressions. 
Xiong et al.~\cite{Xiong:2013:VAM} propose a model-driven framework to diagnose the application performance in cloud condition without manual operation. In the framework, it contains three modules consisted of sensor module, model building module and model updating module. It can automatically detect the workload changes in cloud environment and lead to root cause of performance problem. Cohen et al.~\cite{cohen2004correlating} propose an approach that builds  a promising class of probabilistic models (Tree-Augmented Bayesian Networks or TANs)  to correlate system level counters and systemsâ€™ average-case response time. Cohen et al.~\cite{Cohen:2005:CIC} present that performance counters can successfully be used to construct statistical models for system faults and compact signatures of distinct operational problems. Bodik et al.~\cite{bodik2008hilighter} employ logistic regression with L1 regularization models to construct signatures to improve Cohen et al.\textquotesingle s work.

Multi-models based analysis builds multiple models from performance counters and uses the models to detect performance regressions. Foo et al.~\cite{foo2010mining} propose an approach to detect potential performance regression using association rules. They utilize data mining to extract performance signatures by capturing metrics and employ association rules techniques to collect correlations that are frequently observed in the historical data. Then use the change to the association rules to detect performance anomalies. Jiang et al.~\cite{jiang2009automatic} present two diagnosis algorithms to locate faulty components: RatioScore and SigScore based on component dependencies. They identify the strength of relationships between metric pairs by utilizing an information-theoretic measures  and track system state based on in-cluster entropy. A significant change in the in-cluster entropy is considered as a sign of a performance fault. Shang et al.~\cite{Shang:2015:ADP} propose an approach that first clusters performance metric based on their correlation. Each cluster of metrics is used to build statistical model to detect performance regressions. 

Prior research on performance regressions are designed to be conducted after the system is built and deployed in either performance testing environments or user environments. In this paper, we explore performance regression at commit level, i.e., when the performance regressions are introduced into the software. 

\subsection{Prediction of JIT software defect}
Kamei et al.~\cite{Kamei2013TSE}  present some change measures and builds a logistic regression model to predict just-in-time software defect. The change measures consist of 14 factors derived from six open-source projects and five commercial projects and are grouped into five dimensions. The prediction model the paper builded is able to predict defect-inducing changes with 68 percent accuracy and 64 percent recall. The finding also shows that which part of the factors play more important role in defect-inducing changes. Compared with this paper, we add another factors related performance regression to build the prediction model. Another is that this paper utilizes SZZ algorithm to identify whether or not a change will introduce a defect. However, if the repository does not report the defect we cannot map the defect back to the defect-inducing change. Fukushima et al.~\cite{Fukushima:2014:ESJ} construct Just-in-Time defect prediction cross-project models to identify source code changes that have a high risk of introducing a defect. Zhang et al.~\cite{Zhang:2014:TBU}build a universal defect prediction model for a large set of projects by combining context factors and clustering similar projects to determine the different software metrics sets. In order to find whether or not unsupervised models perform better than the supervised models in effort-aware just-in-time defect prediction, Yang et al.~\cite{Yang:2016:EJD} consider fourteen change metrics and build simple unsupervised and supervised models to predict software defect to determine whether they are of practical value. The results show that many simple unsupervised models perform better than the state-of-the-art supervised models in effort-aware just-in-time defect prediction. Shivaji et al.~\cite{Shivaji2013TSE} realise that the more features (factors) prediction model learned, the more insufficient performance the model predicts, so they perform multiple feature selection algorithms to reduce the factors to predict software bug in a high performance. For the purpose of reducing the number of software failures, Mockus and Weiss ~\cite{mockus2000predicting} are the first one to utilize a number of change measures to build linear regression to predict the probability of failure for a software change. Such change measures include size, duration, diffusion, and type, as well as the experience of the developers who implemented it. However, the discovered defect-inducing changes may be incomplete, which is a potential threat to the correctness of the prediction model. Kamei et al.~\cite{kamei2016studying} construct JIT model based on cross-project models, by using larger pool of training data and combining models from other projects to establish their JIT models, and the result shows performance of within-project model outperforms cross-project model. Tourani et al.~\cite{tourani2016impact}build logistic regression models to study the impact of human discussion metrics on JIT predicting models, result shows a strong correlation between human discussion metrics and defect-prone commits. He et al.~\cite{he2015empirical} study the feasibility of defect-predictor built with simplified metrics in different scenarios, and offer suggestions on choosing datasets and metrics, the result shows the predictor based on minimum metric subset, specific requirements of accuracy and complexity can provide satisfactory performance. Kamei et al.~\cite{kamei2016defect} present an overview in defect prediction field, which intends to introduce and help readers understand previous studies on defect prediction, and highlight some important challenges for future works. Tsakiltsidis et al.~\cite{tsakiltsidis2016automatic} use four machine learning methods to build models to predict performance bugs and the most satisfying model is based on Logistic Regression with all attributes added.

\subsection{Empirical studies on performance}
Empirical studies are conducted in order to study performance issues. Jin et al.~\cite{Jin:2012} studies 109 real world performance issues that are reported from five open source software. Based on the studied 109 performance bugs, Jin et al.~\cite{Jin:2012} develop an automated tool to detect performance issues. Zaman et al.~\cite{MSR11:Zaman,MSR12:Zaman} conducted both qualitative and quantitative studies on performance issues. They find that developers and users face problems in reproducing performance bugs. More time is spent on discussing performance bugs than other kinds of bugs. Huang et al.~\cite{ICSE2014:Huang} studied real world performance issues and based on the findings, they propose an approach called performance risk analysis (PRA), to improve the efficiency of performance regression testing. 

Luo et al.~\cite{ACM2016:Luo} propose a recommendation system, called PerfImpact, to automatically identify code changes that may potentially be responsible for performance regression between two releases. Their approach searches for input values that expose performance regressions and compare execution traces between two releases of a software to identify problematic code changes. Hasan et al.~\cite{Hasan:icse2016} create energy profiles as a performance measurement for different Java collection classes. They find that the energy consumption can have large difference depending on the operation.

Prior studies on performance typically are based on either limited performance issue reports or release of the software. However, the limit amount of issue reports and releases of the software hides the prevalence of performance regressions. In our paper, we evaluate performance at commit level. Therefore, we are able to identify more performance regressions and are able to observe the prevalence of performance regression introducing changes in development. 
